# GVI posteriors in Probabilistic Deep Learning

In this thesis we study probabilistic Deep Learning methods with a focus on Approximate Inference and offer a comparison against state of the art methods including standard Variational Inference, Monte-Carlo Dropout, Stochastic gradient Langevin dynamics and Deep Ensembles. The  focal point of our work is the empirical inspection of the performance of these techniques, compared to a recently proposed framework called Generalised Variational Inference (GVI). GVI can be seen as a generalisation of Bayesian Inference that is speci-fied by an optimisation problem over a space of probability measures with three independent arguments:  a loss, a divergence and a variational family.  By advocating an optimisation-view of Bayesian Modelling, GVI posteriors can be seenas the optimal Q-constrained solution to the optimisation problem.  We motivate GVI especially in the context of Bayesian Neural Networks where,  most of the time, they suffer from certain inappropriate assumptions related to the prior, the likelihood and the computational resources.  We find that, in certain cases, approximate posterior distributions derived from Generalised Variational Inference offer attractive properties with respect to uncertainty quantification,consistency and predictive performance.
